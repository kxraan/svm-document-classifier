{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "#from qpsolvers import solve_qp\n"
      ],
      "metadata": {
        "id": "PlVGfIPH_lYF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKyUBN8xLc5N",
        "outputId": "c1edcd6d-8d4b-4269-ec5f-9cc1b7d25d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "src_path = '/content/drive/My\\ Drive/CMSC422/HW1/20_newsgroups.tar.gz'\n",
        "extract_path = '/content/20_newsgroups'\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "!tar -xzf {src_path} -C {extract_path}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/20_newsgroups/20_newsgroups'\n",
        "\n",
        "newsgroup_classes = os.listdir(dataset_path)\n",
        "print(\"Newsgroup Categories:\", newsgroup_classes)\n",
        "\n",
        "sample_class_path = os.path.join(dataset_path, newsgroup_classes[0])\n",
        "sample_files = os.listdir(sample_class_path)\n",
        "print(\"\\nSample files in category '{}':\".format(newsgroup_classes[0]), sample_files[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALTiCsjVL9De",
        "outputId": "15321619-7b5c-4356-81bd-b41992166837"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Newsgroup Categories: ['rec.sport.baseball', 'sci.med', 'sci.electronics', 'comp.os.ms-windows.misc', 'talk.politics.mideast', 'talk.politics.misc', 'rec.autos', 'talk.politics.guns', 'comp.sys.mac.hardware', 'rec.sport.hockey', 'alt.atheism', 'talk.religion.misc', 'soc.religion.christian', 'comp.windows.x', 'comp.sys.ibm.pc.hardware', 'sci.crypt', 'comp.graphics', 'misc.forsale', 'sci.space', 'rec.motorcycles']\n",
            "\n",
            "Sample files in category 'rec.sport.baseball': ['105087', '104896', '104789', '104775', '105121']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "import os\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def process_all_files(dataset_path):\n",
        "\n",
        "    all_words = []\n",
        "    cleaned_data = {}\n",
        "\n",
        "    for newsgroup in os.listdir(dataset_path):\n",
        "        newsgroup_path = os.path.join(dataset_path, newsgroup)\n",
        "        if os.path.isdir(newsgroup_path):\n",
        "            cleaned_data[newsgroup] = []\n",
        "\n",
        "            for filename in os.listdir(newsgroup_path):\n",
        "                file_path = os.path.join(newsgroup_path, filename)\n",
        "\n",
        "                with open(file_path, 'r', encoding='latin1') as file:\n",
        "                    lines = file.readlines()\n",
        "\n",
        "                # Remove the first four lines\n",
        "                content = ''.join(lines[4:])\n",
        "\n",
        "                # Convert to lowercase\n",
        "                content = content.lower()\n",
        "\n",
        "                # Remove punctuation\n",
        "                content = content.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "                # Split into words\n",
        "                words = content.split()\n",
        "\n",
        "                # Collect words for stop word calculation\n",
        "                all_words.extend(words)\n",
        "\n",
        "\n",
        "                cleaned_data[newsgroup].append((words, file_path))\n",
        "\n",
        "    # Finding the top 200 words as stop words\n",
        "    word_counter = Counter(all_words)\n",
        "    top_300_stop_words = set([word for word, _ in word_counter.most_common(300)])\n",
        "\n",
        "    final_cleaned_data = {}\n",
        "    for newsgroup, files in cleaned_data.items():\n",
        "        final_cleaned_data[newsgroup] = []\n",
        "\n",
        "        for words, file_path in files:\n",
        "            # Remove stop words, short words, and email addresses\n",
        "            words_filtered = [\n",
        "                word for word in words\n",
        "                if word not in top_300_stop_words and\n",
        "                len(word) > 2 and\n",
        "                '@' not in word\n",
        "            ]\n",
        "\n",
        "            # Join filtered words\n",
        "            cleaned_content = ' '.join(words_filtered)\n",
        "\n",
        "            final_cleaned_data[newsgroup].append((cleaned_content, file_path))\n",
        "\n",
        "    return final_cleaned_data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JTlrcHe6MUqH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_dataset = process_all_files(dataset_path)\n",
        "\n",
        "# Display a cleaned file\n",
        "example_class = os.listdir(dataset_path)[0]\n",
        "cleaned_content, file_path = cleaned_dataset[example_class][0]\n",
        "\n",
        "print(f\"\\nSample cleaned email from '{example_class}':\\n\")\n",
        "print(f\"File Path: {file_path}\")\n",
        "print(f\"Content: {cleaned_content[:500]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldxMBDttMtpc",
        "outputId": "5d213146-493a-4173-bd19-32d274753e5a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample cleaned email from 'rec.sport.baseball':\n",
            "\n",
            "File Path: /content/20_newsgroups/20_newsgroups/rec.sport.baseball/105087\n",
            "Content: 13405newsdukeedu 204747 newsnewsdukeedu duke durham keynesecondukeedu term stopper generally refer pitcher counted pitch strong keep team losing streak braves plenty pitchers fit description although expect smoltz glavine mantle braves lack offensive stopper somebody bring hitting slump theres braves rid pure hitter lonnie smith terry pendleton current roster ever shown cursory ability hit worries ron gant slowed step scary slow ron gant econdukeedu flsecondukeedu flsecondukeedu flseconduke corr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(train_dict):\n",
        "    documents = []\n",
        "    labels = []\n",
        "    label_mapping = {label: idx for idx, label in enumerate(train_dict.keys())}\n",
        "\n",
        "    for label, docs in train_dict.items():\n",
        "        for content, _ in docs:\n",
        "            documents.append(content)\n",
        "            labels.append(label_mapping[label])\n",
        "\n",
        "    return np.array(documents), np.array(labels), label_mapping\n"
      ],
      "metadata": {
        "id": "Abp8tuK_pC_m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9mqd1VjXGAlV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf_limited(documents, top_k=500):\n",
        "    N = len(documents)  # Total number of documents\n",
        "    term_frequencies = []  # To store term frequencies for each document\n",
        "    document_frequencies = Counter()  # To count document frequencies for each term\n",
        "\n",
        "    # Calculate term frequencies and document frequencies\n",
        "    for document in documents:\n",
        "        tf = Counter(document.split())\n",
        "        term_frequencies.append(tf)\n",
        "        unique_terms = set(tf.keys())\n",
        "        for term in unique_terms:\n",
        "            document_frequencies[term] += 1\n",
        "\n",
        "    # Get the top-k terms (features) by document frequency\n",
        "    top_k_terms = [term for term, _ in document_frequencies.most_common(top_k)]\n",
        "\n",
        "    # Calculate TF-IDF using the formula for top-k terms only\n",
        "    tfidf_matrix = []\n",
        "    for tf in term_frequencies:\n",
        "        doc_tfidf = {}\n",
        "        for term in top_k_terms:\n",
        "            if term in tf:\n",
        "                tf_value = math.log10(1 + tf[term])\n",
        "                idf_value = math.log10(N / document_frequencies[term])\n",
        "                tfidf = tf_value * idf_value\n",
        "                doc_tfidf[term] = tfidf\n",
        "            else:\n",
        "                doc_tfidf[term] = 0\n",
        "        tfidf_matrix.append(doc_tfidf)\n",
        "\n",
        "    return tfidf_matrix, top_k_terms\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fTdPkKcJjZxv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qpsolvers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMGyjIHwbOwP",
        "outputId": "57bbc474-df1a-48bb-b762-7dc8a7138077"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qpsolvers\n",
            "  Downloading qpsolvers-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from qpsolvers) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from qpsolvers) (1.13.1)\n",
            "Downloading qpsolvers-4.4.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: qpsolvers\n",
            "Successfully installed qpsolvers-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qpsolvers import solve_qp\n",
        "def linear_svm_train(X, y, C=100):\n",
        "\n",
        "    N, D = X.shape\n",
        "\n",
        "    P = np.zeros((D + N + 1, D + N + 1))\n",
        "    P[:D, :D] = np.eye(D)\n",
        "\n",
        "\n",
        "    q = np.hstack([np.zeros(D + 1), C * np.ones(N)])\n",
        "    G = np.zeros((2 * N, D + N + 1))\n",
        "    h = np.zeros(2 * N)\n",
        "\n",
        "    for i in range(N):\n",
        "        G[i, :D] = -y[i] * X[i]\n",
        "        G[i, D] = -y[i]\n",
        "        G[i, D + 1 + i] = -1\n",
        "        h[i] = -1\n",
        "\n",
        "        G[N + i, D + 1 + i] = -1\n",
        "\n",
        "\n",
        "\n",
        "    solution = solve_qp(P, q, G, h, solver=\"cvxopt\")\n",
        "\n",
        "    w = solution[:D]\n",
        "    b = solution[D]\n",
        "\n",
        "    return w, b\n"
      ],
      "metadata": {
        "id": "WZoxrYL9C8AI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_vs_all(X, y, C=100):\n",
        "\n",
        "    classes = np.unique(y)\n",
        "    classifiers = {}\n",
        "\n",
        "    for cls in classes:\n",
        "        print(f\"Training classifier for class {cls}...\")\n",
        "        binary_labels = np.where(y == cls, 1, -1)\n",
        "        w, b = linear_svm_train(X, binary_labels, C=C)\n",
        "        classifiers[cls] = (w, b)\n",
        "\n",
        "    return classifiers\n"
      ],
      "metadata": {
        "id": "0yskjo5SC1yW",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_one_vs_all(classifiers, X):\n",
        "\n",
        "    scores = {}\n",
        "\n",
        "    for cls, (w, b) in classifiers.items():\n",
        "        scores[cls] = X @ w + b\n",
        "\n",
        "    return np.array([max(scores, key=lambda cls: scores[cls][i]) for i in range(X.shape[0])])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qjr287NmD7Mn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spliting the datasets\n",
        "\n",
        "\n",
        "train ={}\n",
        "test = {}\n",
        "\n",
        "for category, documents in cleaned_dataset.items():\n",
        "  train[category] = documents[:500]\n",
        "  test[category] = documents[500:]\n",
        "\n",
        "\n",
        "example_class = list(cleaned_dataset.keys())[0]\n",
        "print(f\"Number of documents in '{example_class}' - Training: {len(train[example_class])}, Testing: {len(test[example_class])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVLcdorTILDn",
        "outputId": "ad4626bd-7c7d-4c16-f321-b8cee9ad73a2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents in 'rec.sport.baseball' - Training: 500, Testing: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_documents, y_train_labels, label_mapping = prepare_data(train)\n",
        "X_train_tfidf, top_500_terms = compute_tfidf_limited(X_train_documents, top_k=500)\n",
        "\n",
        "train_tfidf_df = pd.DataFrame([{term: tfidf.get(term, 0) for term in top_500_terms} for tfidf in X_train_tfidf])\n",
        "\n",
        "X_train_tfidf = train_tfidf_df.to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "classifiers = train_one_vs_all(X_train_tfidf, y_train_labels, C=100)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMZLI5QtIOm6",
        "outputId": "ef7c01d0-98ac-466d-f2c1-99192ae83844"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classifier for class 0...\n",
            "Training classifier for class 1...\n",
            "Training classifier for class 2...\n",
            "Training classifier for class 3...\n",
            "Training classifier for class 4...\n",
            "Training classifier for class 5...\n",
            "Training classifier for class 6...\n",
            "Training classifier for class 7...\n",
            "Training classifier for class 8...\n",
            "Training classifier for class 9...\n",
            "Training classifier for class 10...\n",
            "Training classifier for class 11...\n",
            "Training classifier for class 12...\n",
            "Training classifier for class 13...\n",
            "Training classifier for class 14...\n",
            "Training classifier for class 15...\n",
            "Training classifier for class 16...\n",
            "Training classifier for class 17...\n",
            "Training classifier for class 18...\n",
            "Training classifier for class 19...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_documents, y_test_labels, label_mapping = prepare_data(test)\n",
        "X_test_tfidf, _ = compute_tfidf_limited(X_test_documents, top_k=500)\n",
        "\n",
        "test_tfidf_df = pd.DataFrame([{term: tfidf.get(term, 0) for term in top_500_terms} for tfidf in X_test_tfidf])\n",
        "\n",
        "X_test_tfidf = test_tfidf_df.to_numpy()\n",
        "\n",
        "# Predict using the trained classifiers\n",
        "y_test_pred = predict_one_vs_all(classifiers, X_test_tfidf)\n",
        "print(\"Predicted labels:\", y_test_pred)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfZOG5CAAyUa",
        "outputId": "4d437b5f-27a4-4804-f57c-a2c4c060db73"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels: [14 19  9 ...  8 17 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = sum(y_test_pred == y_test_labels) / len(y_test_labels)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akD1BUQeBc6L",
        "outputId": "d68115ba-b401-40df-dc0c-090443f2bc30"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5568571428571428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def polynomial_kernel(X1, X2):\n",
        "    return np.dot(X1, X2.T) ** 2"
      ],
      "metadata": {
        "id": "z2V5eps8oWoV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cvxopt import matrix, solvers\n",
        "\n",
        "def polynomial_soft_margin_svm(X, y_labels, C=100):\n",
        "    n_samples = X.shape[0]\n",
        "    q = matrix(-np.ones(n_samples))\n",
        "    G = matrix(np.vstack((-np.eye(n_samples), np.eye(n_samples))))\n",
        "    h = matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * C)))\n",
        "\n",
        "    support_data = []\n",
        "\n",
        "    unique_classes = np.unique(y_labels)\n",
        "    for class_label in unique_classes:\n",
        "        print(f\"Training classifier for class {class_label}...\")\n",
        "        y_train = np.where(y_labels == class_label, 1, -1).astype(float)\n",
        "\n",
        "        P = matrix(np.outer(y_train, y_train) * polynomial_kernel(X, X))\n",
        "        A = matrix(y_train, (1, n_samples), 'd')\n",
        "        b = matrix(0.0)\n",
        "\n",
        "        solution = solvers.qp(P, q, G, h, A, b)\n",
        "\n",
        "        alphas = np.ravel(solution['x'])\n",
        "        support_indices = alphas > 1e-5\n",
        "        alphas = alphas[support_indices]\n",
        "        support_vectors = X[support_indices]\n",
        "        support_labels = y_train[support_indices]\n",
        "\n",
        "        support_data.append((alphas, support_vectors, support_labels))\n",
        "\n",
        "    return support_data"
      ],
      "metadata": {
        "id": "zEUueKPtuDkH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_polynomial_svm(X_test, support_data):\n",
        "    predictions = []\n",
        "\n",
        "    for test_point in X_test:\n",
        "        class_scores = []\n",
        "\n",
        "        for alphas, support_vectors, support_labels in support_data:\n",
        "            K_test = polynomial_kernel(support_vectors, test_point.reshape(1, -1)).flatten()\n",
        "            score = np.sum(alphas * support_labels * K_test)\n",
        "            class_scores.append(score)\n",
        "\n",
        "        predictions.append(np.argmax(class_scores))\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "TNBULfVOud2f"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "support_data  = polynomial_soft_margin_svm(X_train_tfidf, y_train_labels, C=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "InzUFguUulEC",
        "outputId": "ec8a9a64-2fad-4bb0-9293-a9ae5672fea5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classifier for class 0...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-efd475d246a8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msupport_data\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpolynomial_soft_margin_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-19f38d1a4bdb>\u001b[0m in \u001b[0;36mpolynomial_soft_margin_svm\u001b[0;34m(X, y_labels, C)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Solve QP problem to find alphas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolvers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Extract and filter alphas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cvxopt/coneprog.py\u001b[0m in \u001b[0;36mqp\u001b[0;34m(P, q, G, h, A, b, solver, kktsolver, initvals, **kwargs)\u001b[0m\n\u001b[1;32m   4483\u001b[0m             'residual as dual infeasibility certificate': dinfres}\n\u001b[1;32m   4484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4485\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconeqp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkktsolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkktsolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cvxopt/coneprog.py\u001b[0m in \u001b[0;36mconeqp\u001b[0;34m(P, q, G, h, dims, A, b, initvals, kktsolver, xnewcopy, xdot, xaxpy, xscal, ynewcopy, ydot, yaxpy, yscal, **kwargs)\u001b[0m\n\u001b[1;32m   2063\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrti\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rti'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrti\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2065\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkktsolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2066\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mArithmeticError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2067\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rank(A) < p or Rank([P; A; G]) < n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cvxopt/coneprog.py\u001b[0m in \u001b[0;36mkktsolver\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m   1979\u001b[0m              \u001b[0mfactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkkt_chol2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1980\u001b[0m          \u001b[0;32mdef\u001b[0m \u001b[0mkktsolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1981\u001b[0;31m              \u001b[0;32mreturn\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1983\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxnewcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxnewcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cvxopt/misc.py\u001b[0m in \u001b[0;36mfactor\u001b[0;34m(W, H, Df)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0;31m# Gs = Wl^{-1} * G.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         base.gemm(spmatrix(W['di'], list(range(ml)), list(range(ml))), \n\u001b[0m\u001b[1;32m   1419\u001b[0m             G, F['Gs'], partial = True)\n\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "y_pred = predict_polynomial_svm(X_test_tfidf, support_data)\n",
        "\n",
        "\n",
        "accuracy = np.mean(y_pred == y_test_labels)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk8R3xjd0Ngm",
        "outputId": "1f03c0c4-7dcf-4965-f80e-37682e5e57df"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7464285714285714\n"
          ]
        }
      ]
    }
  ]
}
